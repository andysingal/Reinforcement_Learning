{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StmTP2EXwnUr"
      },
      "source": [
        "## Twin Delayed DDPG (TD3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5s0R3qxBnTN"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d]==0.21 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    pyglet==1.5.27 \\\n",
        "    pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj6w5kLkz9DN"
      },
      "source": [
        "#### Create the experience buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E18jarh3ybBW"
      },
      "outputs": [],
      "source": [
        "Experience = namedtuple(\n",
        "    \"Experience\",\n",
        "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw-77TRyBvHz"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "      self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "      indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "      states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
        "\n",
        "      return (\n",
        "          np.array(states, dtype=np.float32),\n",
        "          np.array(actions),\n",
        "          np.array(rewards, dtype=np.float32),\n",
        "          np.array(dones, dtype=np.bool),\n",
        "          np.array(next_states, dtype=np.float32)\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtGko6LVQaJz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=200):\n",
        "      self.buffer = buffer\n",
        "      self.sample_size = sample_size\n",
        "\n",
        "  def __iter__(self):\n",
        "      states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
        "      for i in range(len(dones)):\n",
        "          yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkyoL5WQgGm"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ3h8CCOQjGL"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fKGgFzQ4EX"
      },
      "source": [
        "#### Update the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q-OJaPnBvKf"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQMvxcsly9W0"
      },
      "source": [
        "#### Create the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk9xqIipf0Bu"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "    super().__init__()\n",
        "    self.min = torch.from_numpy(min).to(device)\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),           \n",
        "        nn.Linear(hidden_size, out_dims),\n",
        "    )\n",
        "\n",
        "  def mu(self, x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "      x = torch.from_numpy(x).to(device)\n",
        "    return self.net(x)\n",
        "  \n",
        "  def forward(self, x, epsilon=0.0, noise_clip=None):\n",
        "    mu = self.mu(x)\n",
        "    noise = torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "    if noise_clip is not None:\n",
        "      noise = torch.clamp(noise, - noise_clip, noise_clip)\n",
        "    mu = mu + noise\n",
        "    action = torch.max(torch.min(mu, self.max), self.min)\n",
        "    action = action.detach().cpu().numpy()\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBeQSNvzYRk"
      },
      "source": [
        "Create the Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxCtS_Pkzb-4"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size + out_dims, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),           \n",
        "        nn.Linear(hidden_size, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKFUd7jAf0Ej"
      },
      "outputs": [],
      "source": [
        "class TD3(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, capacity=1_000_000, batch_size=256, lr=1e-4, hidden_size=128,\n",
        "               gamma=0.99, loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=5.0, \n",
        "               eps_end=0.2, eps_last_episode=200, samples_per_epoch=10_000, tau=0.01):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "    min_action = self.env.action_space.low\n",
        "\n",
        "\n",
        "    self.q_net1 = DQN(hidden_size, obs_size, action_dims).to(device)\n",
        "    self.q_net2 = DQN(hidden_size, obs_size, action_dims).to(device)\n",
        "    self.policy = GradientPolicy(hidden_size, obs_size, action_dims, min_action, max_action).to(device)\n",
        "\n",
        "    self.target_policy = copy.deepcopy(self.policy)\n",
        "    self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
        "    self.target_q_net2 = copy.deepcopy(self.q_net2)\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      \n",
        "      self.play_episodes(epsilon=self.hparams.eps_start)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episodes(self, policy=None, epsilon=0.):\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        if policy:\n",
        "          action = self.policy(obs, epsilon=epsilon)\n",
        "        else:\n",
        "          action = self.env.action_space.sample()\n",
        "          \n",
        "        next_obs, reward, done, info = self.env.step(action)\n",
        "        exp = Experience(obs, action, reward, done, next_obs)\n",
        "        self.buffer.append(exp)\n",
        "        obs = next_obs\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.policy(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_parameters = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "    q_net_optimizer = self.hparams.optim(q_net_parameters, lr=self.hparams.lr)\n",
        "    policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer, policy_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    if optimizer_idx == 0:\n",
        "\n",
        "      epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "      )\n",
        "\n",
        "      state_action_values1 = self.q_net1(states, actions)\n",
        "      state_action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "      target_actions = self.target_policy(next_states, epsilon=epsilon, noise_clip=0.5)\n",
        "\n",
        "      next_state_values = torch.min(\n",
        "          self.target_q_net1(next_states, target_actions),\n",
        "          self.target_q_net2(next_states, target_actions)\n",
        "      )\n",
        "\n",
        "      next_state_values[dones] = 0.0\n",
        "\n",
        "      expected_state_action_values = rewards + self.hparams.gamma * next_state_values\n",
        "      q_loss1 = self.hparams.loss_fn(state_action_values1, expected_state_action_values)\n",
        "      q_loss2 = self.hparams.loss_fn(state_action_values2, expected_state_action_values)\n",
        "      total_loss = q_loss1 + q_loss2\n",
        "      self.log(\"episode/MSE Loss\", total_loss)\n",
        "      return total_loss\n",
        "\n",
        "    elif optimizer_idx == 1 and batch_idx % 2 == 0:\n",
        "      policy_loss = - self.q_net1(states, self.policy.mu(states)).mean()\n",
        "      self.log(\"episode/Policy Loss\", policy_loss)\n",
        "      return policy_loss\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    self.play_episodes(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "    polyak_average(self.q_net1, self.target_q_net1, tau=self.hparams.tau)\n",
        "    polyak_average(self.q_net2, self.target_q_net2, tau=self.hparams.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "    self.log(\"episode/Episode return\", self.env.return_queue[-1], prog_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1b2qnsACnz4"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rn7R1Sff0HS"
      },
      "outputs": [],
      "source": [
        "algo = TD3('LunarLanderContinuous-v2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus, \n",
        "    max_epochs=2_000, \n",
        "    track_grad_norm=2,\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd3Qvne4xbHX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}